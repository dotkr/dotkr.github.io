---
title: 2024 하계 모각코 5주차
layout: post
date: '2024-08-07 21:00:00'
icon: github
category: mgc
---

## 모각코 5주차
### 😀학번, 이름 : 202102557, 이현
### 🎡계획 : 자연어 처리 복습


### 📄결과 :
- 자연어 처리 복습
- RNN 구조 복습

##### RNN
- RNN은 순환 신경망으로, 순차적인 데이터를 처리하는데 사용된다.
- RNN은 이전의 정보를 기억하고, 현재의 입력에 따라 다음 출력을 결정한다.
- RNN은 시퀀스 데이터를 처리하는데 적합하다.
- RNN은 시퀀스 데이터의 길이에 상관없이 처리할 수 있다.
- RNN은 시퀀스 데이터의 패턴을 학습할 수 있다.
- 다만, RNN은 시퀀스가 길어질수록 학습이 어려워진다.
- 왜나하면, RNN은 시퀀스가 길어질수록 이전의 정보를 기억하기 어렵기 때문이다.

![RNN](/post-img/mgc/2024/week5_0.png)

---

##### RNN의 단점을 해결 하기 위해 여러 방안이 제시되었다.
- LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)가 그 대표적인 방안이다.


- LSTM은 RNN의 장기 의존성 문제를 해결하기 위해 고안되었다.
- LSTM은 RNN의 기본 구조에 셀 상태(cell state)와 게이트(gate)를 추가하여 장기 의존성 문제를 해결한다.
- LSTM은 셀 상태를 통해 장기 의존성을 유지하고, 게이트를 통해 정보의 흐름을 제어한다.
- LSTM은 RNN의 장기 의존성 문제를 해결하면서, 단기 의존성 문제도 해결한다.


- GRU는 LSTM의 복잡한 구조를 간소화한 모델이다.
- GRU는 LSTM의 셀 상태와 게이트를 합쳐서 업데이트 게이트와 리셋 게이트로 구성된다.
- GRU는 LSTM보다 간단한 구조로, 학습 속도가 빠르다.
- GRU는 LSTM과 비슷한 성능을 보이지만, LSTM보다 간단한 구조로 인해 학습 속도가 빠르다.
- GRU는 LSTM보다 적은 파라미터를 사용하므로, 메모리 사용량이 적다.

![LSTM, GRU](/post-img/mgc/2024/week5_1.png)
